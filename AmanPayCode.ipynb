{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vZVJziVF4LT",
        "outputId": "ef3e4941-7e06-497c-aa00-82c990463d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating data... please wait.\n",
            "âœ… Success! Generated 5100 transactions in 'amanpay_transactions.csv'.\n",
            "Check your folder, the file should be there.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "NUM_NORMAL = 5000\n",
        "NUM_FRAUD = 100  # These are the \"Hidden\" scams\n",
        "OUTPUT_FILE = \"amanpay_transactions.csv\"\n",
        "\n",
        "# --- HELPER FUNCTIONS ---\n",
        "def generate_normal_transaction():\n",
        "    \"\"\"Simulates a student buying food/transport in Kampar/KL\"\"\"\n",
        "    categories = ['Food', 'Transport', 'Telco', 'Grocery', 'Entertainment']\n",
        "\n",
        "    # Students spend small amounts (RM 5 to RM 50)\n",
        "    amount = round(np.random.normal(20, 10), 2)\n",
        "    if amount < 2: amount = 5.00 # Minimum spend\n",
        "\n",
        "    # Normal hours (8 AM to 11 PM)\n",
        "    hour = random.randint(8, 23)\n",
        "\n",
        "    # Local Coordinates (Kampar/KL area)\n",
        "    # Kampar Lat: 4.3 approx, KL Lat: 3.1 approx\n",
        "    lat = round(random.uniform(3.1, 4.5), 4)\n",
        "    lon = round(random.uniform(101.0, 101.8), 4)\n",
        "\n",
        "    return {\n",
        "        \"amount\": amount,\n",
        "        \"hour\": hour,\n",
        "        \"category\": random.choice(categories),\n",
        "        \"lat\": lat,\n",
        "        \"lon\": lon,\n",
        "        \"is_fraud\": 0  # 0 means Safe\n",
        "    }\n",
        "\n",
        "def generate_fraud_transaction():\n",
        "    \"\"\"Simulates a scammer draining the account\"\"\"\n",
        "    # Scammers steal BIG amounts or oddly specific amounts\n",
        "    scam_type = random.choice(['DRAIN', 'PHISHING'])\n",
        "\n",
        "    if scam_type == 'DRAIN':\n",
        "        amount = round(random.uniform(2000, 5000), 2)\n",
        "    else:\n",
        "        amount = 199.00 # Phishing often uses repeated exact numbers\n",
        "\n",
        "    # Weird hours (2 AM to 5 AM)\n",
        "    hour = random.randint(0, 5)\n",
        "\n",
        "    # Suspicious Coordinates (Overseas or far away)\n",
        "    # e.g., Macau, Nigeria, or just far North\n",
        "    lat = round(random.uniform(10.0, 50.0), 4)\n",
        "    lon = round(random.uniform(110.0, 120.0), 4)\n",
        "\n",
        "    return {\n",
        "        \"amount\": amount,\n",
        "        \"hour\": hour,\n",
        "        \"category\": \"Transfer\",\n",
        "        \"lat\": lat,\n",
        "        \"lon\": lon,\n",
        "        \"is_fraud\": 1  # 1 means Fraud\n",
        "    }\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "print(\"Generating data... please wait.\")\n",
        "\n",
        "data = []\n",
        "\n",
        "# 1. Create Normal Data\n",
        "for _ in range(NUM_NORMAL):\n",
        "    data.append(generate_normal_transaction())\n",
        "\n",
        "# 2. Inject Fraud Data\n",
        "for _ in range(NUM_FRAUD):\n",
        "    data.append(generate_fraud_transaction())\n",
        "\n",
        "# 3. Save to CSV\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Shuffle the data so scams are mixed in\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "df.to_csv(OUTPUT_FILE, index=False)\n",
        "print(f\"âœ… Success! Generated {len(df)} transactions in '{OUTPUT_FILE}'.\")\n",
        "print(\"Check your folder, the file should be there.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "baeZyia0GEhr",
        "outputId": "271d332b-f773-4beb-fde2-1449f0767c69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      amount  hour       category     lat       lon  is_fraud\n",
              "0      14.61     9  Entertainment  3.9441  101.5490         0\n",
              "1      22.95    21           Food  4.1324  101.6243         0\n",
              "2      29.10    11      Transport  3.5038  101.4755         0\n",
              "3      20.36    17  Entertainment  3.2106  101.5820         0\n",
              "4      10.89    22          Telco  4.3735  101.6450         0\n",
              "...      ...   ...            ...     ...       ...       ...\n",
              "5095   11.93    23  Entertainment  3.8429  101.2497         0\n",
              "5096    3.45     9      Transport  4.1266  101.7998         0\n",
              "5097   26.24     8           Food  4.0592  101.6472         0\n",
              "5098   25.62    11        Grocery  3.2565  101.0822         0\n",
              "5099   23.90     9          Telco  3.8544  101.3594         0\n",
              "\n",
              "[5100 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f6ac4499-987a-4fc9-a5eb-279346466f58\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>amount</th>\n",
              "      <th>hour</th>\n",
              "      <th>category</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>is_fraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.61</td>\n",
              "      <td>9</td>\n",
              "      <td>Entertainment</td>\n",
              "      <td>3.9441</td>\n",
              "      <td>101.5490</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22.95</td>\n",
              "      <td>21</td>\n",
              "      <td>Food</td>\n",
              "      <td>4.1324</td>\n",
              "      <td>101.6243</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29.10</td>\n",
              "      <td>11</td>\n",
              "      <td>Transport</td>\n",
              "      <td>3.5038</td>\n",
              "      <td>101.4755</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20.36</td>\n",
              "      <td>17</td>\n",
              "      <td>Entertainment</td>\n",
              "      <td>3.2106</td>\n",
              "      <td>101.5820</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.89</td>\n",
              "      <td>22</td>\n",
              "      <td>Telco</td>\n",
              "      <td>4.3735</td>\n",
              "      <td>101.6450</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5095</th>\n",
              "      <td>11.93</td>\n",
              "      <td>23</td>\n",
              "      <td>Entertainment</td>\n",
              "      <td>3.8429</td>\n",
              "      <td>101.2497</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5096</th>\n",
              "      <td>3.45</td>\n",
              "      <td>9</td>\n",
              "      <td>Transport</td>\n",
              "      <td>4.1266</td>\n",
              "      <td>101.7998</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5097</th>\n",
              "      <td>26.24</td>\n",
              "      <td>8</td>\n",
              "      <td>Food</td>\n",
              "      <td>4.0592</td>\n",
              "      <td>101.6472</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5098</th>\n",
              "      <td>25.62</td>\n",
              "      <td>11</td>\n",
              "      <td>Grocery</td>\n",
              "      <td>3.2565</td>\n",
              "      <td>101.0822</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5099</th>\n",
              "      <td>23.90</td>\n",
              "      <td>9</td>\n",
              "      <td>Telco</td>\n",
              "      <td>3.8544</td>\n",
              "      <td>101.3594</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5100 rows Ã— 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f6ac4499-987a-4fc9-a5eb-279346466f58')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f6ac4499-987a-4fc9-a5eb-279346466f58 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f6ac4499-987a-4fc9-a5eb-279346466f58');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_fb4193c1-e447-493a-a448-f840356bc8c6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_fb4193c1-e447-493a-a448-f840356bc8c6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5100,\n  \"fields\": [\n    {\n      \"column\": \"amount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 374.1982705925483,\n        \"min\": 2.0,\n        \"max\": 4994.44,\n        \"num_unique_values\": 2676,\n        \"samples\": [\n          13.26,\n          16.21,\n          21.18\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hour\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 23,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          9,\n          15,\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Entertainment\",\n          \"Food\",\n          \"Transfer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.889862613653474,\n        \"min\": 3.1,\n        \"max\": 49.2695,\n        \"num_unique_values\": 4295,\n        \"samples\": [\n          3.1067,\n          3.3838,\n          3.212\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lon\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.9257011482604212,\n        \"min\": 101.0,\n        \"max\": 119.8671,\n        \"num_unique_values\": 3863,\n        \"samples\": [\n          101.668,\n          101.0638,\n          101.587\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_fraud\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "INPUT_FILE = \"amanpay_transactions.csv\"\n",
        "OUTPUT_FILE = \"amanpay_cleaned.csv\"\n",
        "\n",
        "def clean_data():\n",
        "    print(\"ğŸ§¹ Starting Data Cleaning Process...\")\n",
        "\n",
        "    # 1. Load the Data\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_FILE)\n",
        "        print(f\"   -> Loaded {len(df)} rows.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"âŒ Error: generate_data.py hasn't been run yet!\")\n",
        "        return\n",
        "\n",
        "    # 2. Check for Missing Values (and remove them)\n",
        "    if df.isnull().sum().sum() > 0:\n",
        "        print(f\"   -> Found missing values. Removing...\")\n",
        "        df = df.dropna()\n",
        "    else:\n",
        "        print(\"   -> No missing values found. (Good!)\")\n",
        "\n",
        "    # 3. Remove Duplicates\n",
        "    # In real banking, duplicates might happen due to system glitches\n",
        "    initial_count = len(df)\n",
        "    df = df.drop_duplicates()\n",
        "    final_count = len(df)\n",
        "\n",
        "    if initial_count > final_count:\n",
        "        print(f\"   -> Removed {initial_count - final_count} duplicate rows.\")\n",
        "    else:\n",
        "        print(\"   -> No duplicates found.\")\n",
        "\n",
        "    # 4. Data Type Conversion (Optimization)\n",
        "    # Ensure 'amount' is a number (float)\n",
        "    df['amount'] = pd.to_numeric(df['amount'], errors='coerce')\n",
        "\n",
        "    # 5. Save the Cleaned Data\n",
        "    df.to_csv(OUTPUT_FILE, index=False)\n",
        "    print(\"------------------------------------------------\")\n",
        "    print(f\"Data Cleaning Complete!\")\n",
        "    print(f\"   -> Saved cleaned data to: '{OUTPUT_FILE}'\")\n",
        "    print(\"------------------------------------------------\")\n",
        "\n",
        "    # Show a quick summary for the User\n",
        "    print(\"\\nData Preview (First 5 rows):\")\n",
        "    print(df.head())\n",
        "    print(\"\\nStatistics:\")\n",
        "    print(df.describe())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    clean_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ad-OkheGHfF",
        "outputId": "6e5001e9-80a8-4913-f726-f116257c743a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§¹ Starting Data Cleaning Process...\n",
            "   -> Loaded 5100 rows.\n",
            "   -> No missing values found. (Good!)\n",
            "   -> No duplicates found.\n",
            "------------------------------------------------\n",
            "Data Cleaning Complete!\n",
            "   -> Saved cleaned data to: 'amanpay_cleaned.csv'\n",
            "------------------------------------------------\n",
            "\n",
            "Data Preview (First 5 rows):\n",
            "   amount  hour       category     lat       lon  is_fraud\n",
            "0   14.61     9  Entertainment  3.9441  101.5490         0\n",
            "1   22.95    21           Food  4.1324  101.6243         0\n",
            "2   29.10    11      Transport  3.5038  101.4755         0\n",
            "3   20.36    17  Entertainment  3.2106  101.5820         0\n",
            "4   10.89    22          Telco  4.3735  101.6450         0\n",
            "\n",
            "Statistics:\n",
            "            amount         hour          lat          lon     is_fraud\n",
            "count  5100.000000  5100.000000  5100.000000  5100.000000  5100.000000\n",
            "mean     59.089286    15.217647     4.290316   101.659175     0.019608\n",
            "std     374.198271     4.931546     3.889863     1.925701     0.138662\n",
            "min       2.000000     0.000000     3.100000   101.000000     0.000000\n",
            "25%      12.887500    11.000000     3.444375   101.199450     0.000000\n",
            "50%      20.270000    15.000000     3.802350   101.398550     0.000000\n",
            "75%      27.380000    19.000000     4.160925   101.602475     0.000000\n",
            "max    4994.440000    23.000000    49.269500   119.867100     1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, losses\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "INPUT_FILE = \"amanpay_cleaned.csv\"\n",
        "MODEL_FILE = \"amanpay_model.keras\"  # The saved \"Brain\"\n",
        "SCALER_FILE = \"scaler.pkl\"          # The saved \"Translator\"\n",
        "\n",
        "# 1. Load the Data\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(INPUT_FILE)\n",
        "\n",
        "# We only care about these columns for the AI\n",
        "features = ['amount', 'hour', 'lat', 'lon']\n",
        "data = df[features]\n",
        "\n",
        "# 2. Preprocess (Scale the numbers)\n",
        "# AI works better when numbers are small (between 0 and 1)\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# 3. Split Data\n",
        "# We train ONLY on Normal data (is_fraud = 0)\n",
        "# This is called \"Anomaly Detection\"\n",
        "normal_data = data_scaled[df['is_fraud'] == 0]\n",
        "train_data, test_data = train_test_split(normal_data, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training on {len(train_data)} normal transactions...\")\n",
        "\n",
        "# 4. Build the TensorFlow Model (Autoencoder)\n",
        "# It tries to compress the data and then recreate it.\n",
        "# If it fails to recreate a transaction, that transaction is a SCAM.\n",
        "input_dim = data_scaled.shape[1] # 4 features\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  # Encoder (Compress)\n",
        "  layers.Dense(8, activation=\"relu\", input_shape=(input_dim,)),\n",
        "  layers.Dense(4, activation=\"relu\"),\n",
        "\n",
        "  # Decoder (Expand)\n",
        "  layers.Dense(8, activation=\"relu\"),\n",
        "  layers.Dense(input_dim, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "# 5. Train the Model\n",
        "history = model.fit(train_data, train_data,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(test_data, test_data),\n",
        "                    shuffle=True,\n",
        "                    verbose=1)\n",
        "\n",
        "# 6. Save the Brain and Scaler\n",
        "model.save(MODEL_FILE)\n",
        "with open(SCALER_FILE, 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "print(\"Success! Model trained and saved.\")\n",
        "print(f\"Files created: {MODEL_FILE}, {SCALER_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8cOLnn3GOvj",
        "outputId": "de0cb225-3c7f-4ef3-c746-0da52f334181"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Training on 4000 normal transactions...\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.6734 - val_loss: 0.5277\n",
            "Epoch 2/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4468 - val_loss: 0.2738\n",
            "Epoch 3/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2510 - val_loss: 0.2270\n",
            "Epoch 4/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2187 - val_loss: 0.2229\n",
            "Epoch 5/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2161 - val_loss: 0.2212\n",
            "Epoch 6/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2167 - val_loss: 0.2204\n",
            "Epoch 7/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2167 - val_loss: 0.2200\n",
            "Epoch 8/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2120 - val_loss: 0.2197\n",
            "Epoch 9/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2173 - val_loss: 0.2195\n",
            "Epoch 10/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2157 - val_loss: 0.2193\n",
            "Epoch 11/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2175 - val_loss: 0.2191\n",
            "Epoch 12/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2138 - val_loss: 0.2189\n",
            "Epoch 13/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2169 - val_loss: 0.2188\n",
            "Epoch 14/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2119 - val_loss: 0.2186\n",
            "Epoch 15/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2138 - val_loss: 0.2184\n",
            "Epoch 16/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2135 - val_loss: 0.2184\n",
            "Epoch 17/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2139 - val_loss: 0.2181\n",
            "Epoch 18/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2131 - val_loss: 0.2177\n",
            "Epoch 19/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2146 - val_loss: 0.2173\n",
            "Epoch 20/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2121 - val_loss: 0.2170\n",
            "Success! Model trained and saved.\n",
            "Files created: amanpay_model.keras, scaler.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHyFXn7CHI1b",
        "outputId": "d5d453a1-9d35-4725-a9ef-dc9a3d6ddec2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.53.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=5.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.46)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.26.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2026.1.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.53.0-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.53.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import db\n",
        "import time\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "MODEL_FILE = \"amanpay_model.keras\"\n",
        "SCALER_FILE = \"scaler.pkl\"\n",
        "FIREBASE_KEY = \"key.json\"  # Ensure you uploaded this file!\n",
        "\n",
        "# PASTE YOUR GOOGLE GEMINI API KEY HERE\n",
        "GOOG_API_KEY = \"YOUR_API_KEY\"\n",
        "\n",
        "# --- 2. SETUP FIREBASE ---\n",
        "# We use a \"Try/Except\" block to prevent errors when Streamlit reloads\n",
        "if not firebase_admin._apps:\n",
        "    try:\n",
        "        cred = credentials.Certificate(FIREBASE_KEY)\n",
        "        firebase_admin.initialize_app(cred, {\n",
        "            'databaseURL': 'OWN_FIREBASE_LINK'\n",
        "            # ^^^ IMPORTANT: REPLACE THIS URL WITH YOUR OWN FIREBASE URL IF IT FAILS ^^^\n",
        "        })\n",
        "        st.toast(\"ğŸ”¥ Firebase Connected!\", icon=\"â˜ï¸\")\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Firebase not connected: {e}\")\n",
        "\n",
        "# --- 3. AUTO-DETECT GEMINI MODEL ---\n",
        "gemini_active = False\n",
        "model_gemini = None\n",
        "try:\n",
        "    if GOOG_API_KEY.startswith(\"AIza\"):\n",
        "        genai.configure(api_key=GOOG_API_KEY)\n",
        "        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]\n",
        "        chosen_model = next((m for m in available_models if \"gemini\" in m), available_models[0] if available_models else None)\n",
        "        if chosen_model:\n",
        "            model_gemini = genai.GenerativeModel(chosen_model)\n",
        "            gemini_active = True\n",
        "except:\n",
        "    gemini_active = False\n",
        "\n",
        "# --- 4. LOAD BRAIN ---\n",
        "@st.cache_resource\n",
        "def load_resources():\n",
        "    if not os.path.exists(MODEL_FILE): return None, None\n",
        "    model = tf.keras.models.load_model(MODEL_FILE)\n",
        "    with open(SCALER_FILE, 'rb') as f: scaler = pickle.load(f)\n",
        "    return model, scaler\n",
        "\n",
        "try:\n",
        "    model, scaler = load_resources()\n",
        "    if model is None: st.stop()\n",
        "except: st.stop()\n",
        "\n",
        "# --- 5. UI LAYOUT ---\n",
        "st.set_page_config(page_title=\"AmanPay AI\", page_icon=\"ğŸ›¡ï¸\", layout=\"wide\")\n",
        "st.title(\"ğŸ›¡ï¸ AmanPay AI\")\n",
        "st.markdown(\"### Securing the Digital Economy for Malaysiaâ€™s Underserved\")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# --- 6. SIDEBAR INPUTS ---\n",
        "st.sidebar.header(\"ğŸ“ Transaction Simulator\")\n",
        "amount = st.sidebar.number_input(\"Amount (RM)\", min_value=1.0, value=20.0, step=10.0)\n",
        "hour = st.sidebar.slider(\"Time of Day (24h)\", 0, 23, 14)\n",
        "st.sidebar.subheader(\"ğŸ“ Location Details\")\n",
        "lat = st.sidebar.number_input(\"Latitude\", value=3.1408, format=\"%.4f\")\n",
        "lon = st.sidebar.number_input(\"Longitude\", value=101.6932, format=\"%.4f\")\n",
        "btn = st.sidebar.button(\"ğŸš€ Analyze Transaction\", type=\"primary\")\n",
        "\n",
        "# --- 7. MAIN LOGIC ---\n",
        "if btn:\n",
        "    # Scale & Predict\n",
        "    input_data = pd.DataFrame([[amount, hour, lat, lon]], columns=['amount', 'hour', 'lat', 'lon'])\n",
        "    input_scaled = scaler.transform(input_data)\n",
        "    reconstructed = model.predict(input_scaled)\n",
        "    error_score = np.mean(np.power(input_scaled - reconstructed, 2), axis=1)[0]\n",
        "    is_fraud = error_score > 0.5\n",
        "\n",
        "    # === FIREBASE LOGGING ===\n",
        "    # This sends the data to the cloud instantly\n",
        "    try:\n",
        "        ref = db.reference(\"/transactions\")\n",
        "        ref.push({\n",
        "            \"amount\": amount,\n",
        "            \"hour\": hour,\n",
        "            \"risk_score\": float(error_score),\n",
        "            \"status\": \"FRAUD\" if is_fraud else \"SAFE\",\n",
        "            \"timestamp\": time.time()\n",
        "        })\n",
        "        st.toast(\"Data synced to Cloud Database\", icon=\"âœ…\")\n",
        "    except Exception as e:\n",
        "        print(f\"Firebase Error: {e}\")\n",
        "\n",
        "    # === DISPLAY RESULTS ===\n",
        "    col1, col2 = st.columns([2, 1])\n",
        "    with col1:\n",
        "        st.subheader(\"ğŸ“Š Analysis Result\")\n",
        "        if is_fraud:\n",
        "            st.error(f\"ğŸš¨ FRAUD DETECTED!\")\n",
        "            st.metric(\"Risk Score\", f\"{error_score:.4f}\", \"High Risk\", delta_color=\"inverse\")\n",
        "\n",
        "            st.markdown(\"#### ğŸ¤– AI Guardian Explanation:\")\n",
        "            if gemini_active:\n",
        "                with st.spinner(\"Analyzing context...\"):\n",
        "                    try:\n",
        "                        prompt = f\"Act as a security bot. A user spent RM {amount} at hour {hour}. Location: {lat}, {lon}. Risk Score: {error_score:.2f}. Explain professionally in English why this is blocked in 1 sentence.\"\n",
        "                        response = model_gemini.generate_content(prompt)\n",
        "                        st.info(response.text)\n",
        "                    except: st.error(\"AI Busy.\")\n",
        "        else:\n",
        "            st.success(f\"âœ… Transaction Approved\")\n",
        "            st.metric(\"Risk Score\", f\"{error_score:.4f}\", \"Safe\")\n",
        "\n",
        "    with col2:\n",
        "        st.subheader(\"ğŸŒ Location Tracker\")\n",
        "        st.map(pd.DataFrame({'lat': [lat], 'lon': [lon]}))\n",
        "\n",
        "# --- 8. LIVE HISTORY SECTION ---\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"â˜ï¸ Live Cloud Database Records\")\n",
        "try:\n",
        "    # Read the last 5 transactions from Firebase\n",
        "    ref = db.reference(\"/transactions\")\n",
        "    snapshot = ref.order_by_key().limit_to_last(5).get()\n",
        "\n",
        "    if snapshot:\n",
        "        # Convert Firebase JSON to a nice Table\n",
        "        data_list = []\n",
        "        for key, val in snapshot.items():\n",
        "            data_list.append(val)\n",
        "        df_history = pd.DataFrame(data_list)\n",
        "        # Reorder columns for neatness\n",
        "        st.dataframe(df_history[['status', 'amount', 'risk_score', 'hour']], use_container_width=True)\n",
        "    else:\n",
        "        st.write(\"No records in cloud yet.\")\n",
        "except:\n",
        "    st.write(\"Connect Firebase to see history.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG_hXlLCHPLx",
        "outputId": "8f105b30-a1e2-4ceb-df72-da9132ecd78a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import db\n",
        "import time\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "MODEL_FILE = \"amanpay_model.keras\"\n",
        "SCALER_FILE = \"scaler.pkl\"\n",
        "FIREBASE_KEY = \"key.json\"  # Ensure you uploaded this file!\n",
        "\n",
        "# !!! PASTE YOUR GOOGLE GEMINI API KEY HERE !!!\n",
        "GOOG_API_KEY = \"AIzaSyA4j5xl1lkGEKx3LTTYsTJMCs9q6FNpDkE\"\n",
        "\n",
        "# --- 2. SETUP FIREBASE ---\n",
        "# We use a \"Try/Except\" block to prevent errors when Streamlit reloads\n",
        "if not firebase_admin._apps:\n",
        "    try:\n",
        "        cred = credentials.Certificate(FIREBASE_KEY)\n",
        "        firebase_admin.initialize_app(cred, {\n",
        "            'databaseURL': 'https://aipay-61b05-default-rtdb.asia-southeast1.firebasedatabase.app/'\n",
        "            # ^^^ IMPORTANT: REPLACE THIS URL WITH YOUR OWN FIREBASE URL IF IT FAILS ^^^\n",
        "        })\n",
        "        st.toast(\"ğŸ”¥ Firebase Connected!\", icon=\"â˜ï¸\")\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Firebase not connected: {e}\")\n",
        "\n",
        "# --- 3. AUTO-DETECT GEMINI MODEL ---\n",
        "gemini_active = False\n",
        "model_gemini = None\n",
        "try:\n",
        "    if GOOG_API_KEY.startswith(\"AIza\"):\n",
        "        genai.configure(api_key=GOOG_API_KEY)\n",
        "        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]\n",
        "        chosen_model = next((m for m in available_models if \"gemini\" in m), available_models[0] if available_models else None)\n",
        "        if chosen_model:\n",
        "            model_gemini = genai.GenerativeModel(chosen_model)\n",
        "            gemini_active = True\n",
        "except:\n",
        "    gemini_active = False\n",
        "\n",
        "# --- 4. LOAD BRAIN ---\n",
        "@st.cache_resource\n",
        "def load_resources():\n",
        "    if not os.path.exists(MODEL_FILE): return None, None\n",
        "    model = tf.keras.models.load_model(MODEL_FILE)\n",
        "    with open(SCALER_FILE, 'rb') as f: scaler = pickle.load(f)\n",
        "    return model, scaler\n",
        "\n",
        "try:\n",
        "    model, scaler = load_resources()\n",
        "    if model is None: st.stop()\n",
        "except: st.stop()\n",
        "\n",
        "# --- 5. UI LAYOUT ---\n",
        "st.set_page_config(page_title=\"AmanPay AI\", page_icon=\"ğŸ›¡ï¸\", layout=\"wide\")\n",
        "st.title(\"ğŸ›¡ï¸ AmanPay AI\")\n",
        "st.markdown(\"### Securing the Digital Economy for Malaysiaâ€™s Underserved\")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# --- 6. SIDEBAR INPUTS ---\n",
        "st.sidebar.header(\"ğŸ“ Transaction Simulator\")\n",
        "amount = st.sidebar.number_input(\"Amount (RM)\", min_value=1.0, value=20.0, step=10.0)\n",
        "hour = st.sidebar.slider(\"Time of Day (24h)\", 0, 23, 14)\n",
        "st.sidebar.subheader(\"ğŸ“ Location Details\")\n",
        "lat = st.sidebar.number_input(\"Latitude\", value=3.1408, format=\"%.4f\")\n",
        "lon = st.sidebar.number_input(\"Longitude\", value=101.6932, format=\"%.4f\")\n",
        "btn = st.sidebar.button(\"ğŸš€ Analyze Transaction\", type=\"primary\")\n",
        "\n",
        "# --- 7. MAIN LOGIC ---\n",
        "if btn:\n",
        "    # Scale & Predict\n",
        "    input_data = pd.DataFrame([[amount, hour, lat, lon]], columns=['amount', 'hour', 'lat', 'lon'])\n",
        "    input_scaled = scaler.transform(input_data)\n",
        "    reconstructed = model.predict(input_scaled)\n",
        "    error_score = np.mean(np.power(input_scaled - reconstructed, 2), axis=1)[0]\n",
        "    is_fraud = error_score > 0.5\n",
        "\n",
        "    # === FIREBASE LOGGING ===\n",
        "    # This sends the data to the cloud instantly\n",
        "    try:\n",
        "        ref = db.reference(\"/transactions\")\n",
        "        ref.push({\n",
        "            \"amount\": amount,\n",
        "            \"hour\": hour,\n",
        "            \"risk_score\": float(error_score),\n",
        "            \"status\": \"FRAUD\" if is_fraud else \"SAFE\",\n",
        "            \"timestamp\": time.time()\n",
        "        })\n",
        "        st.toast(\"Data synced to Cloud Database\", icon=\"âœ…\")\n",
        "    except Exception as e:\n",
        "        print(f\"Firebase Error: {e}\")\n",
        "\n",
        "    # === DISPLAY RESULTS ===\n",
        "    col1, col2 = st.columns([2, 1])\n",
        "    with col1:\n",
        "        st.subheader(\"ğŸ“Š Analysis Result\")\n",
        "        if is_fraud:\n",
        "            st.error(f\"ğŸš¨ FRAUD DETECTED!\")\n",
        "            st.metric(\"Risk Score\", f\"{error_score:.4f}\", \"High Risk\", delta_color=\"inverse\")\n",
        "\n",
        "            st.markdown(\"#### ğŸ¤– AI Guardian Explanation:\")\n",
        "            if gemini_active:\n",
        "                with st.spinner(\"Analyzing context...\"):\n",
        "                    try:\n",
        "                        prompt = f\"Act as a security bot. A user spent RM {amount} at hour {hour}. Location: {lat}, {lon}. Risk Score: {error_score:.2f}. Explain professionally in English why this is blocked in 1 sentence.\"\n",
        "                        response = model_gemini.generate_content(prompt)\n",
        "                        st.info(response.text)\n",
        "                    except: st.error(\"AI Busy.\")\n",
        "        else:\n",
        "            st.success(f\"âœ… Transaction Approved\")\n",
        "            st.metric(\"Risk Score\", f\"{error_score:.4f}\", \"Safe\")\n",
        "\n",
        "    with col2:\n",
        "        st.subheader(\"ğŸŒ Location Tracker\")\n",
        "        st.map(pd.DataFrame({'lat': [lat], 'lon': [lon]}))\n",
        "\n",
        "# --- 8. LIVE HISTORY SECTION ---\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"â˜ï¸ Live Cloud Database Records\")\n",
        "try:\n",
        "    # Read the last 5 transactions from Firebase\n",
        "    ref = db.reference(\"/transactions\")\n",
        "    snapshot = ref.order_by_key().limit_to_last(5).get()\n",
        "\n",
        "    if snapshot:\n",
        "        # Convert Firebase JSON to a nice Table\n",
        "        data_list = []\n",
        "        for key, val in snapshot.items():\n",
        "            data_list.append(val)\n",
        "        df_history = pd.DataFrame(data_list)\n",
        "        # Reorder columns for neatness\n",
        "        st.dataframe(df_history[['status', 'amount', 'risk_score', 'hour']], use_container_width=True)\n",
        "    else:\n",
        "        st.write(\"No records in cloud yet.\")\n",
        "except:\n",
        "    st.write(\"Connect Firebase to see history.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG_B8Yu2HWVa",
        "outputId": "c6bab54b-8de8-4b50-d2bd-30af95a6f018"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 cloudflared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UIHfAJxJys4",
        "outputId": "dc4503ef-eace-4b10-8dc5-61baa00f43f1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-16 02:40:35--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2025.11.1/cloudflared-linux-amd64 [following]\n",
            "--2026-01-16 02:40:35--  https://github.com/cloudflare/cloudflared/releases/download/2025.11.1/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/106867604/955e9d1b-ac5e-4188-8867-e5f53958a8fe?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-01-16T03%3A40%3A25Z&rscd=attachment%3B+filename%3Dcloudflared-linux-amd64&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-01-16T02%3A39%3A52Z&ske=2026-01-16T03%3A40%3A25Z&sks=b&skv=2018-11-09&sig=fMG7WDZgS1LJ9zFmNcyh%2B4VUSAbAa%2Fp0uq7FYwC%2Fst0%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2ODUzMzAzNSwibmJmIjoxNzY4NTMxMjM1LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.B-AfKrZBOc2fANhk7U2qEBKnOucfD4wzsox9Tg8UYiQ&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2026-01-16 02:40:35--  https://release-assets.githubusercontent.com/github-production-release-asset/106867604/955e9d1b-ac5e-4188-8867-e5f53958a8fe?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-01-16T03%3A40%3A25Z&rscd=attachment%3B+filename%3Dcloudflared-linux-amd64&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-01-16T02%3A39%3A52Z&ske=2026-01-16T03%3A40%3A25Z&sks=b&skv=2018-11-09&sig=fMG7WDZgS1LJ9zFmNcyh%2B4VUSAbAa%2Fp0uq7FYwC%2Fst0%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2ODUzMzAzNSwibmJmIjoxNzY4NTMxMjM1LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.B-AfKrZBOc2fANhk7U2qEBKnOucfD4wzsox9Tg8UYiQ&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41258324 (39M) [application/octet-stream]\n",
            "Saving to: â€˜cloudflared-linux-amd64â€™\n",
            "\n",
            "cloudflared-linux-a 100%[===================>]  39.35M   193MB/s    in 0.2s    \n",
            "\n",
            "2026-01-16 02:40:36 (193 MB/s) - â€˜cloudflared-linux-amd64â€™ saved [41258324/41258324]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill streamlit\n",
        "!streamlit run app.py & ./cloudflared tunnel --url http://localhost:8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQX6d6NHJ54S",
        "outputId": "d48ab1a5-a8c7-418a-e36e-5f89ed2ae7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2026-01-16T02:40:42Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2026-01-16T02:40:42Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.11.170.89:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m |  https://fotos-ltd-pam-pose.trycloudflare.com                                              |\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 991dffd8889ee9f0147b6b48933da9e4407e68ea8c6d984f55fa2d3db4bb431d)\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 22450423-23e1-45ac-8f6b-09fd7827bf78\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.227\n",
            "2026/01/16 02:40:45 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2026-01-16T02:40:45Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0me287d15f-0e1a-4d98-8d8d-27031154d200 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.227 \u001b[36mlocation=\u001b[0msea01 \u001b[36mprotocol=\u001b[0mquic\n"
          ]
        }
      ]
    }
  ]
}